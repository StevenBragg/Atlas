Self-Organizing Audio-Visual Learning System Architecture


Architecture Overview: Extending to Audio-Visual Inputs


We propose a biologically inspired, self-organizing architecture that learns from a raw, synchronized audio-video stream without any pretraining or labeled data. This design extends the previous video-only system to incorporate an additional audio pathway, while adhering to the same principles of local learning and self-organization. The system treats the incoming camera frames and microphone waveform as raw physical signals, with no built-in notions of objects, speech, or other human-centric features. It must discover structure autonomously through exposure. Key characteristics of the architecture include:

No Backpropagation or Task Bias: All learning uses local synaptic plasticity rules (Hebbian learning, Oja’s rule, Spike-Timing-Dependent Plasticity) operating on correlations and spike timings . There is no end-to-end error gradient or human-defined objective; the system’s only “teacher” is the temporal structure of the sensory stream itself.

Modular Sensory Pathways: Separate video and audio processing pathways handle each modality’s input, initially learning independent representations. Each pathway consists of layers of neurons that self-organize their connectivity and tuning based on statistical regularities in that modality.

Cross-Modal Integration: As learning progresses, the system forms bridges between modalities through local associations. Neurons across the two pathways develop connections if their patterns of activity consistently coincide. This allows the network to gradually align audio and visual representations for the same environmental events (when appropriate) or keep them distinct when they are uncorrelated.

Emergent Encoding, Memory, and Prediction: The architecture is designed such that higher-level layers act as an internal memory for temporal patterns. Neurons learn to encode not just static features, but also to predict upcoming sensory input (next video frame, next audio segment, or both) based on recent activity. Over time, the network builds an internal model of the world, enabling it to anticipate what comes next in the stream.

Stability and Diversity: Mechanisms are included to preserve the stability of learning and prevent collapse of the representation. These include weight normalization and homeostatic scaling to keep neurons from saturating , sparsity-promoting dynamics (e.g. inhibitory competition or winner-take-all) to encourage a diverse set of features , and structural plasticity to grow or prune connections as needed .

Structural Plasticity: The network can rewire and expand itself by adding new neurons or synapses in response to novel patterns, and pruning those that are redundant. This mimics brain development, where new synaptic connections are sprouted or removed based on experience . It ensures the capacity to learn new patterns without catastrophic interference, while keeping the model efficient.



Figure 1 below illustrates the high-level architecture. It shows parallel unimodal streams for vision and audition feeding into a shared associative memory. All learning is local to each synapse (no global error signals). Initially, the visual and auditory parts learn separately. Later, cross-modal links form in the convergence zone where multimodal neurons or synapse links bind co-occurring patterns. This convergence-divergence principle is inspired by neuroscience: despite disparate senses like sight and sound, brains form unified representations (convergence) and can activate one modality’s representation from another’s cue (divergence) .



(If figure were available, it would depict two input streams (frames and audio waveform) feeding into respective hierarchical feature layers, which then connect to a joint layer. The joint layer feeds back to both modalities for prediction.)



Unimodal Sensory Processing with Local Plasticity


Visual Pathway: Self-Organizing Video Encoding


The visual pathway remains as in the video-only design, providing a foundation for unsupervised image sequence learning. Raw video frames (e.g. pixel arrays from the webcam) feed into a hierarchy of neuron layers that progressively extract structured visual features. Each layer is trained with local Hebbian-like rules to respond to statistical regularities in its input. Key aspects of the video pathway:

Layer 1 (Low-Level Vision): Neurons receive raw pixel inputs (or small patches) and strengthen connections for pixels that often activate together (Hebb’s rule). With appropriate constraints like competition and normalization, these neurons specialize to detect basic visual primitives – for example, oriented edges, contrasts, or color blobs – without any pre-defined filters. Indeed, simple Hebbian rules combined with normalization can extract principal components of images , and more sophisticated local learning (e.g. sparse Hebbian or ICA) yields Gabor-like edge detectors as seen in biological V1. By relying on Oja’s rule (a normalized Hebbian update), each neuron’s weight vector is constrained to unit length, preventing unbounded growth and focusing it on a dominant visual feature axis . Lateral inhibition (neurons competing to respond) ensures that different neurons pick up different edge orientations or locations, covering the input space.

Layer 2+ (Higher Visual Features): Outputs of Layer 1 feed into higher layers that capture combinations or temporal groupings of the lower-level features. For example, a Layer 2 neuron might receive inputs from a set of edge detectors that frequently activate in succession or in a particular spatial arrangement, thus learning to recognize a contour or texture pattern. All learning remains local: when a Layer 2 neuron is active at the same time as certain Layer 1 neurons, those synapses strengthen (Hebbian association). At the same time, synapses from features that are rarely co-active with the neuron may fade or be pruned to maintain sparsity. Over time, the visual hierarchy builds a distributed representation of the scene, with early layers akin to retinotopic feature maps and higher layers capturing more complex, invariant properties (e.g. a neuron might become selective to a specific recurring shape or motion pattern in the video). Crucially, no concept of what the pattern “means” is assumed – the neuron might fire for a certain combination of pixel motion and color because it has found that combination statistically salient, not because it knows anything about objects or gestures.

Temporal Context and Prediction in Vision: Within the visual pathway, recurrent connections or a context layer may be present to learn the temporal continuity of features. Using STDP (Spike-Timing-Dependent Plasticity), if one visual neuron tends to fire shortly before another, a synapse from the first to the second is potentiated . This effectively chains visual features into predictive sequences. For instance, if an angled edge moving upward is often followed by a particular corner shape (maybe as an object rotates), the network will reinforce connections so that the edge-detector neuron excites the corner-detector neuron with a slight delay. These temporal associations allow the visual system to predict the next set of features from the current ones. In practice, after learning, the onset of a familiar motion pattern in Layer 1 will cue Layer 2 neurons that normally come next, effectively anticipating the visual outcome. This implements a form of predictive coding locally: each layer tries to predict the activation of the next input it will see, and any mismatch drives further adaptation. Neuroscientific models of predictive coding suggest each layer of cortex projects predictions to lower layers and only unpredicted residuals get passed upward . Here, without explicit error feedback, the temporal Hebbian links implicitly encode those predictions – if the prediction was correct, the expected neuron fires and the Hebbian link is reinforced; if not, that link doesn’t get reinforced (and may weaken due to lack of co-activation).



Overall, the visual pathway provides a self-organized visual feature encoder and temporal predictor. By the time the network has observed enough video, it has a stable set of visual neurons representing the frequently occurring spatial patterns and their typical temporal transitions. These form the basis for later cross-modal association.



Auditory Pathway: Segmentation and Local Encoding of Audio


In parallel to the vision stream, an audio processing pathway learns to encode raw sound waves from the microphone into meaningful internal features. This pathway is designed to mirror the visual pathway’s organization, but with appropriate transformations for time-series audio data. Important design elements for the auditory stream include:

Input Preprocessing (Auditory Segmentation): The raw audio waveform (a 1-D time signal) is first broken into manageable time slices or frames. This segmentation could be synchronized to video frames (e.g. 30–60 audio windows per second matching the frame rate), or use a small sliding window (e.g. 10–20 ms) to capture short acoustic events. The goal is to present the neural network with snapshots of sound over short durations, analogous to how a video frame is a snapshot of vision. Each audio slice is treated as a high-dimensional vector of sound pressure samples. In addition, a bank of band-pass filters (simulating a cochlea) can decompose the audio into frequency channels. For example, a filter bank covering low to high frequencies will produce a spectral vector for each time window, indicating which frequencies are present. This yields a cochleagram or spectrogram-like representation as input to the first auditory layer. Using a frequency decomposition is biologically motivated and gives the network a distributed representation (different neurons can specialize to different frequency bands). However, even this filter bank is not tuned to any specific pattern – it’s a generic Fourier-like analysis that still requires learning to interpret.

Layer 1 (Low-Level Auditory Features): Neurons in the first auditory layer receive some combination of time-localized signals (either raw waveform segments or multi-band spectrogram slices). Learning in this layer follows Hebbian plasticity and competition just as in vision. Neurons that happen to “hear” particular patterns (combinations of frequencies or temporal dynamics) repeatedly will strengthen synapses for those patterns. For instance, one neuron might become a tone detector – if a pure frequency around 440 Hz often occurs, a neuron connecting to the corresponding filter channel will fire and reinforce that connection, eventually responding reliably to that tone. Another neuron might pick up a broadband onset – e.g. whenever a sudden burst of wide-frequency noise occurs (like a clap or click), all frequency channels spike briefly; a neuron capturing that coincidence across the spectrum will become sensitive to any sharp transient. Yet another neuron might detect a frequency combination (like two tones that often occur together). Essentially, through Hebbian association the layer finds recurring spectral patterns. Competitive learning (e.g. winner-take-all where only the most activated neuron learns for a given input) ensures different neurons specialize: one neuron focuses on a certain frequency band, another on a different band or time-pattern, so they don’t all redundantly learn the loudest frequency. As with vision, Oja’s rule or weight normalization is applied per neuron to prevent runaway growth – each neuron’s weight vector (across the auditory input channels) is constrained to unit length or otherwise normalized after each Hebbian update . This keeps the neuron from trivially increasing all weights and responding to just overall volume; instead it must pick specific patterns to respond to. Biological auditory neurons similarly adapt their frequency tuning based on experience; experiments show that with exposure, neurons can alter their “preferred frequencies” when certain tone combinations are repeatedly paired (indicating unsupervised re-tuning via STDP or similar rules).

Layer 2 (Higher Auditory Features): The next auditory layer receives inputs from groups of Layer 1 neurons (or from a time series of Layer 1 activations) and can learn more complex or invariant features. For example, a Layer 2 neuron might take input from several tone-detector neurons that often fire in a sequence (forming a simple melody or harmonic progression) and thus learn to recognize that sequence as a unit. Another might combine a transient broadband neuron with a low-frequency tone neuron if those often occur together (perhaps corresponding to a specific timbre onsets). All of this is learned through the same principle: neurons that fire together wire together. If a certain combination of Layer 1 activations coincides often in time, a Layer 2 neuron’s synapses to those active inputs are potentiated. In effect, the auditory system can develop analogs of phoneme-like or sound-object-like detectors without any prior knowledge of language or events – it might discover frequently co-occurring frequency patterns which could correspond to environmental sounds (like the noise of a moving object, or a particular musical note), though it does not label them as such.

Temporal Sequencing in Audio: The auditory stream is inherently temporal, so we incorporate mechanisms for sequence learning here as well. Neurons can maintain a short-term memory trace of their past firing (e.g. through synaptic eligibility traces or recurrent connections). If neuron A fires and shortly after neuron B fires, the synapse A→B is strengthened (classic STDP: presynaptic before postsynaptic leads to potentiation) . This means if a particular sound pattern is often followed by another, the network will link those representations. Over time, the auditory pathway becomes adept at predicting what sound comes next: hearing the beginning of a familiar sequence will pre-activate neurons for the likely continuation. For instance, if the sound of a rolling object is usually followed by a thud, the representation of “rolling” will trigger the “thud” neuron via their learned connection. This sequence prediction is entirely unsupervised and local: the timing correlations in the input drive the strengthening of predictive connections. Notably, this is analogous to how the visual sequence memory works – the same learning rule is applied to temporal patterns of audio features.



Through these stages, the auditory pathway self-organizes a set of features and temporal predictions independent of the visual pathway. It essentially builds an “auditory model of the world,” capturing the recurring frequencies, rhythms, and temporal transitions in the sound stream. Research in unsupervised neural coding shows that the same learning strategies can apply across modalities – for instance, sparse Hebbian or ICA-based learning yields Gabor filters for vision and Gammatone-like filters for audio, simply by changing the input data . This supports our approach of designing analogous architectures for image and sound: both use local learning to find efficient codes and both enforce sparsity/independence. At this stage, the visual and auditory representations are separate: the network has discovered its own visual features and its own auditory features, but it does not yet know which ones correspond to the same physical event. That cross-modal binding is the next step.



Cross-Modal Association and Joint Structure Discovery


Once the system has some footing in each modality, we enable cross-modal connections so it can learn the statistical relationships between sight and sound. The guiding principle is that consistent temporal coincidences between a visual pattern and an auditory pattern will cause an association to form, whereas unrelated patterns remain unlinked. This mirrors how infants learn that certain sights and sounds go together (e.g. a bouncing ball and a bouncing sound), without being told – simply by experiencing them together repeatedly. Our architecture implements this via local Hebbian links between the modalities and/or a shared multimodal layer:

Convergence Zone or Multimodal Layer: At the top of the modality-specific hierarchies, we introduce a multimodal association layer where neurons receive inputs from both the visual feature set and the auditory feature set. This can be thought of as a “convergence zone” : neurons here have synapses coming from high-level visual neurons and from high-level auditory neurons. Initially, these synapses are untrained (small random weights). When a particular visual feature neuron and a particular auditory feature neuron are simultaneously active (e.g. the sight of a pattern coincides with a sound), any multimodal neuron that receives inputs from both will experience coincident input and, by Hebb’s rule, will strengthen those synapses. Through repeated exposure, that multimodal neuron becomes tuned to that specific audio-visual conjunction. For example, suppose one visual neuron fires for a certain moving shape, and an auditory neuron fires for a certain buzzing sound; if these occur together often, a multimodal neuron may wire itself to both, effectively becoming a detector for the combination “that shape and that sound occurring together.” Importantly, this neuron would remain inactive if only the shape occurs without the sound or vice versa (unless the association is so strong that one input alone can partially activate it). Over time, a population of multimodal neurons will form, each capturing different recurring audio-visual coincidences.

Direct Hebbian Links Between Modalities: In addition to (or instead of) dedicated multimodal neurons, we can allow direct synaptic links between the existing auditory and visual neurons. If an auditory neuron B and a visual neuron A frequently fire around the same time, we create a bidirectional connection A ↔ B. This is a form of structural plasticity: the network grows a new connection that wasn’t originally wired, because a statistical relationship was discovered . Each co-occurrence strengthens the synapse (visual→auditory and/or auditory→visual), binding the two representations into an assembly. The next time A fires, it will tend to activate B, and vice versa, even if the other modality’s input is absent. In effect, the visual neuron has learned to “predict” the auditory neuron and can excite it (and again, Hebbian updates have no notion of semantic meaning – this is purely because A and B were correlated in time). Biological evidence supports such cross-modal Hebbian wiring: if separate sensory maps are already formed, STDP can learn mappings between them, achieving multimodal integration . This is analogous to how neurons in the superior colliculus or associative cortex link visual and auditory cues that originate from the same event.

Temporal Window and Alignment: We incorporate temporal sensitivity in these associations. Real-world audio and visual signals may not start and stop in perfect lockstep – there could be delays (sound often slightly lags visual events, for example). By using STDP-like rules for cross-modal synapses, the system can account for slight lead/lag relationships. For instance, if visual neuron A tends to fire just before auditory neuron B (like seeing an impact then hearing a bang), the A→B synapse will strengthen even if A’s spike precedes B’s by some milliseconds (since STDP potentiates a synapse when presynaptic activity comes slightly before postsynaptic) . The opposite direction (B→A) might also form if occasionally the order is reversed, but typically one direction will dominate if one modality reliably leads the other. This creates directed associations that effectively encode which modality can cue the other. In practice, the network learns the synchrony and lag structure of audio-visual events: truly synchronous events (like a lip movement with immediate sound) produce symmetric links, whereas events with a fixed lag produce asymmetric predictive links.

Maintaining Separate Representations: Not all visual features will find a counterpart in audio, and vice versa. The system should preserve modality-specific representations when appropriate. We ensure that cross-modal binding occurs only for statistically significant correlations, by requiring a minimum co-activation frequency before a link is strongly cemented. Weak or spurious coincidences will result in either no new synapse or a very weak one that may eventually be pruned if not reinforced. For example, the visual neuron for “sunlight flicker” might sometimes coincide with a loud sound purely by chance; unless this pairing repeats consistently, the network will treat it as unrelated. This way, the auditory and visual pathways retain independent features for aspects of the environment that do not cross modalities. The learning rule essentially performs a form of clustering by mutual information: it links signals that share information (i.e., reduce uncertainty about each other) and leaves independent signals unlinked. The result is that some higher-level neurons become multimodal (responding to combined audio-visual patterns), while others remain unimodal specialists for patterns unique to one sense.

Reentrant Loops (Bidirectional Association): The connections between modalities are set up to be bidirectional, enabling reentrant processing . That is, when a learned audio-visual association is activated from one side, it can activate the other side as well. If a known visual pattern occurs, its associated auditory neuron may fire even if the actual sound is not present, effectively recalling or predicting the sound (a form of mental imagery or expectation). Conversely, hearing a known sound could activate the visual neurons that usually accompany it, making the system internally “imagine” the visual pattern. This phenomenon of one modality triggering another’s representation is observed in the brain (e.g., seeing silent lip movements can elicit auditory cortex activity expecting the sound ). In our system, this emerges naturally from the bidirectional Hebbian links – a partial cue activates the whole linked assembly (the network becomes “both a detector and a producer of signals” after association, as noted in a reentrant model ). These reentrant connections essentially create a distributed memory of multimodal events: a fragment (either sensory cue) can retrieve the rest of the event’s representation from memory.



By discovering joint structure in the multimodal input, the system moves from independent sensory coding to an integrated audio-visual representation. It is important to stress that this integration is self-supervised and grounded in the data: the system does not know what a particular audio-visual pairing means (it has no concept of “a glass breaking” or “a spoken word and the mouth shape”), but it has built a predictive link between certain patterns of light and sound because they frequently occur together.



Physically, after training, we might observe neural assemblies that correspond to environmental phenomena. For instance, one assembly might link a particular visual motion pattern with a particular frequency burst – possibly corresponding to a specific action with its sound. Another assembly might join a complex visual scene pattern with silence (if it learned something like a visual pattern that usually occurs in quiet). The system’s representations are entirely emergent – perhaps akin to how an infant’s brain might start to align the sight of a parent’s face with the sound of their voice over repeated exposures, without any innate knowledge of “voice” or “face”.



Temporal Memory and Prediction Across Modalities


A core function of the system is to model the temporal dynamics of the world, enabling it to predict future sensory input. We extend the prediction mechanism to the multimodal setting, meaning the network learns to anticipate what will happen next in both the visual and auditory streams. Prediction is crucial for self-supervised learning: by attempting to predict the next moment and comparing it to what actually occurs, the system can self-tune its representations to better match reality. Here’s how prediction is handled:

Modality-Specific Prediction: Each sensory pathway (visual and auditory) is equipped to predict the next input in its own modality. As described earlier, recurrent connections or temporal synapses within each pathway allow it to learn sequences of patterns. Concretely, neurons in higher layers act as context units that carry information from the recent past. Suppose at time t a set of visual neurons fire representing the current frame. Through learned connections, these active neurons will tend to activate another set of visual neurons that historically follow at time t+1. Thus, the network generates an internal prediction of the next frame’s feature activity. Similarly, the auditory pathway generates an internal prediction of the next audio window’s features. These predictions are not output as explicit signals, but rather manifest as pre-activation or increased readiness of the expected neurons. When time t+1 arrives and actual sensory input comes in, the already primed neurons fire more easily if the prediction was correct, or unexpected neurons fire if there was something novel. The difference between predicted and actual firing patterns inherently influences synaptic changes: correctly predicted co-firings get reinforced (Hebbian strengthening), while unexpected activations trigger new learning (new connections form or different neurons learn this surprise). In this way, prediction errors locally drive learning – the network adjusts so that future similar surprises can be better predicted.

Joint Multimodal Prediction: Beyond separate predictions, the associative connections enable cross-modal predictive cues. Because audio and visual events are linked in memory, prediction can propagate from one modality to the other. For example, if a certain visual pattern tends to be followed by a certain sound, once the visual part of that pattern is recognized at time t, the auditory neuron (or multimodal neuron) for the associated sound may become activated in advance. This effectively predicts the sound before it arrives. The system could thus anticipate an audio event purely from visual context. Conversely, hearing a familiar sound may allow the system to predict an ensuing visual change. In practice, the multimodal layer or cross-links serve as a bridge for prediction: the state of the multimodal neurons at time t represents an expectation of the complete sensory scene in the near future. One implementation is to have a recurrent associative memory that operates over the joint state (audio+visual) – for instance, an auto-associative Hopfield-like network or a recurrent SOM that, when given part of a pattern (just the visual part), can retrieve the full pattern (audio+visual) as the prediction . As time advances, this retrieved pattern is compared to actual input. Through structural plasticity (adding connections), the memory network can store sequences of multimodal patterns, allowing it to not only associate synchronous events but also sequential events across time.

Next-Frame Audio-Visual Predictions: The ultimate outcome is that the system can attempt to predict the entire next sensory frame – both the next video frame’s features and the next audio snippet’s features. One can imagine at time t, the network’s high-level representation (including multimodal neurons and context from time t-1) generates a hypothesized pattern for time t+1. This hypothesis consists of a set of visual feature neurons that should fire and a set of auditory feature neurons that should fire. These predictions trickle down via feedback connections: e.g. a multimodal neuron that is predicting a certain combination might send top-down signals to lower visual and auditory layers to pre-activate the expected lower-level features. In a predictive coding framework, these top-down signals act as priors, and the feed-forward input at t+1 is matched against them . Any mismatch (error) is then a local signal that triggers synaptic adjustment. In our design, we accomplish a similar effect with local rules by treating prediction as just another association over time: the network strengthens the connections that correctly predicted actual activity (since those neurons fired as expected together), and weakens or fails to reinforce those that predicted wrongly (mismatched activity does not produce Hebbian reinforcement). Over repeated trials, the network converges to a set of connections that encode the statistical transitions of the sensory stream.

Dealing with Uncertainty: The world is not fully deterministic, so our system must handle uncertainty in predictions. Rather than predicting one fixed outcome, the network’s state can represent a pattern of possible outcomes. For instance, if usually after A comes B or C with equal probability, the memory may partially activate both B and C neurons upon seeing A. The actual next input will then solidify one of them (reinforcing A→that one). The other association (A→the alternative) might still be kept if it occurs often enough on other occasions. Neurons thus effectively encode a distribution of possibilities. This is akin to the brain’s predictive coding where predictions are probabilistic and errors refine beliefs. We ensure that learning rules (being local) do not force a single outcome in ambiguous cases; they simply strengthen whichever outcome happened. Over time the synaptic strengths will reflect the frequencies of outcomes, meaning the network’s predictive state can represent multiple expectancies weighted by their likelihood.

Prediction of Both Streams vs Either: The question of whether to predict audio, video or both can be addressed by having separate prediction error signals in each modality that modulate plasticity. The system can try to minimize video prediction error and audio prediction error simultaneously. If one modality is easier to predict from the other (e.g. video might predict audio better than audio predicts video for certain events or vice versa), those cross-modal connections will become stronger since they consistently reduce error. In essence, the network doesn’t explicitly choose to predict one or the other; it attempts to predict everything it can. This holistic approach means sometimes the visual context drives audio prediction, sometimes audio context drives visual, and often each modality predicts itself. The architecture could include an internal loop where predicted next-frame sensory signals are generated and then compared with actual next-frame – any surprise triggers a flurry of synaptic updates at the points of discrepancy. This keeps learning self-correcting and always driven by the stream.



Through these predictive mechanisms, our self-organizing system builds a form of world model that spans both sight and sound. It will anticipate synchronized audio-visual events as one unit, and unsynchronized or independent events separately. The predictive ability is a sign that the system has captured temporal structures: we can monitor decreasing prediction errors as a measure of learning progress (discussed below). Furthermore, because prediction is done without any external training signal (purely via the sensory data), it adheres to the requirement of no supervised or task-specific bias – the temporal continuity of the sensory world itself is the supervision. This approach aligns with mounting evidence that prediction is a fundamental unsupervised learning rule in the brain .



Mechanisms for Stability, Sparsity, and Diversity


Unsupervised Hebbian learning, if unchecked, can lead to unstable outcomes – e.g. a few neurons might greedily increase their weights and dominate, or the network might fall into trivial representations (like all neurons learning the same frequent pattern). To ensure stable learning and rich representations, we incorporate several biologically inspired mechanisms:

Homeostatic Plasticity: Each neuron maintains a target firing rate and adjusts its excitability or thresholds to stay around that rate. If a neuron is rarely active, its threshold can lower or its synaptic gains increase, making it easier to activate (so it can start participating in representation). If it’s too active (fires for too many inputs), its threshold rises or its synapses scale down, to prevent it from hogging all inputs. This principle, seen in the BCM rule (Bienenstock–Cooper–Munro) where the synaptic modification threshold slides based on average activity, ensures competition and specialization: neurons that have learned important features become harder to excite further (forcing diminishing returns) and underutilized neurons become more sensitive until they find a niche. Such homeostatic adjustments are essential because pure Hebbian rules lack any negative feedback . In fact, without regulation, continuous Hebbian learning can cause runaway excitation or all synapses saturating . By introducing homosynaptic scaling (per-synapse) and heterosynaptic scaling (across all synapses of a neuron) , the network keeps synaptic weights in a healthy range. One simple implementation is periodically normalizing the weights of each neuron (mean-center and normalize variance) , which was shown to stabilize STDP-based feature learning in multilayer networks .

Hebbian Normalization (Oja’s Rule): As mentioned, Oja’s rule provides an elegant way to normalize Hebbian learning on the fly. For a given neuron with weight vector w, Oja’s update rule is: Δw = η (y * x – y^2 * w), where x is the input vector and y is the neuron’s output. The second term (–y^2w) effectively renormalizes the weight magnitude . This keeps the neuron’s weight vector from growing without bound. As a result, each neuron tends to learn a principal component of the input distribution rather than something unbounded. Using Oja’s rule in early layers helps the network achieve an efficient coding of the sensory input, distributing different principal components among different neurons. It also inherently prevents a collapse where one feature gets amplified infinitely. In practice, we apply Oja’s rule or a similar weight normalization after each Hebbian increment in both auditory and visual layers. This way, if a neuron starts responding to a very common pattern (which might otherwise cause it to fire constantly and strengthen all its weights), the normalization will limit its influence, ensuring room for other neurons to respond to other aspects of the data.

Lateral Inhibition and Winner-Take-All: We incorporate competitive dynamics so that only a subset of neurons respond to any given input. For example, in each layer, when an input pattern is presented, we can allow the top N scoring neurons to fire (k-Winners-Take-All) or have neurons inhibit their neighbors proportional to their activation. This approximates the sparse coding observed in cortex and forces specialization. If one neuron is currently tuned perfectly to the input, it will fire and suppress others, and possibly learn a bit more, but if the input is slightly different than any neuron’s tuning, whichever neuron is closest will fire and then adjust its weights to better fit – or if none are close, a new neuron might activate (see structural plasticity below). Winner-Take-All STDP models have been shown to successfully train multi-layer networks of locally learned features, yielding sparse convolutional filters useful for classification . The WTA mechanism was key in those models for stability, as it selects the “most relevant patches to learn from” and serves as a form of attention focusing plasticity on one neuron at a time . We use a similar idea: for each small region of the input (or each time step), only one or a few neurons adapt strongly, thereby avoiding the scenario where many neurons redundantly learn the same pattern. This not only preserves diversity of features (each neuron tries to win for different inputs) but also helps with sparsity (only a few active at once, mimicking brain sparsity levels).

Sparse Activity and Inhibitory Circuits: In addition to WTA, we maintain overall network sparsity via inhibitory interneuron models. For instance, if too many neurons in a layer activate together, a global inhibitory feedback can dampen all of them (akin to a “balance” that keeps total activity roughly constant). The target could be, say, only 5% of neurons active at any time. This is again a form of homeostatic control but at the population level. Sparse coding is known to improve representation quality and prevent representational collapse, as it encourages the network to use a distributed code (each input activates a unique combination of a few neurons, rather than all inputs activating everything). We might implement this by adjusting each neuron’s firing threshold such that a fixed fraction of neurons fire for any given input (the threshold adapts to input statistics). This ensures that no input ends up activating all neurons (which would be a failure of specificity), and no neuron remains inactive for too long.

Synaptic Competition and Pruning: As part of structural stability, synapses that are rarely if ever used are slowly weakened and removed. Each neuron may start with potential connections to many inputs, but after sufficient learning, it retains only those that were useful. In the Reentrant SOM model, they explicitly pruned up to 90% of possible connections, keeping only the strongest after multimodal training . We adopt a similar approach continuously: if a synapse’s weight remains near zero and doesn’t get potentiated in a long time, the connection can be pruned to free up capacity (and possibly allow new synapses elsewhere). This prevents noise buildup and focuses the network on consistent patterns. It also helps avoid overfitting to transient coincidences. Pruning combined with sprouting (described next) lets the connectivity graph self-optimize.

Avoiding Collapse of Diversity: Representational collapse would mean many neurons converge to encoding the same feature or a few features, losing coverage of the input space. Our use of competition, homeostasis, and the sheer richness of the input (multimodal data has many degrees of freedom) fights against collapse. Additionally, we can monitor the correlation between neuron activities – if we detect that two neurons are highly correlated in output for all inputs, they are redundant. In such a case, one might reduce its learning rate or be repurposed. We prefer a scenario where each neuron has a unique profile of activity across the dataset (high selectivity). Techniques like decorrelation (subtracting a small fraction of the mean activity of other neurons from a neuron’s input) can be introduced to encourage neurons to respond to novel aspects that others do not. In a biological sense, lateral inhibitory connections often create competition in tuning so that neurons evenly cover, say, different orientations or frequencies (a phenomenon observed in sensory cortex maps). STDP in recurrent networks has even been shown to create topologically organized maps spontaneously , which implies neurons spread out their feature preferences rather than all collapsing to one. We leverage that tendency by allowing lateral connections (either inhibitory or excitatory) within a layer to self-organize as well, forming perhaps a soft topographic map of features.



By combining these mechanisms, the system remains robust and adaptable. It will not blow up its weights thanks to normalization; it won’t go silent or hyperactive thanks to homeostatic rate control; it won’t have all neurons doing the same thing thanks to competitive learning; and it won’t freeze too early thanks to gradual structural updates and continued plasticity for new patterns. The outcome is a stable set of diverse feature detectors in each modality and effective multimodal associations, maintained over long durations of learning.



Structural Plasticity: Growing and Rewiring the Network


In a complex, open-ended environment, the system may encounter new patterns or increased complexity that demands more resources or new connections. To handle this, we incorporate structural plasticity – the ability of the network to change its architecture by adding or removing neurons and synapses in response to the data. This ensures the network can expand its representational capacity when needed, without human intervention, and also simplify itself if certain components prove unnecessary. Key strategies for structural plasticity include:

New Neuron Recruitment: Initially, we might start with a modest number of neurons in each layer. If the sensory stream presents patterns that none of the existing neurons can adequately model (for example, a completely novel combination of features that consistently triggers high prediction error or very broad activation), the network should allocate new neurons to represent that pattern. We can implement this by monitoring the error or activation patterns: when an input pattern yields no “winner” neuron with activation above a threshold (meaning the network doesn’t recognize it), a new neuron is added to that layer with random initial weights (or weights initialized to that input pattern). This neuron will then adapt via plasticity to specialize on that pattern. This is akin to algorithms like Growing Neural Gas or evolving SOMs, where new units are added for poorly represented regions of the input space. Structural growth must be done sparingly and with stability in mind – for instance, require that the novel pattern reappears multiple times and consistently evades existing neurons before triggering addition. Over time, this allows the system to increase its complexity in response to environment complexity, rather than being stuck with a fixed capacity.

Synapse Sprouting: Similarly, for connectivity between layers or across modalities, if two neurons show a strong correlation but have no direct connection, we can sprout a new synapse linking them. This was discussed in the multimodal association (sprouting links between auditory and visual neurons that co-occur). We generalize it: any time a neuron A in one region is frequently coincident with neuron B in another region (or even in the same layer if lateral connections are allowed) and if current synaptic pathways don’t connect A to B, we can create a new synapse A→B. This allows the network to form new pathways to convey information that it discovers is important. For example, if a high-level visual neuron and a high-level auditory neuron consistently fire together for a new type of event, a direct link can be formed to bind that event. Sprouting needs a structural limit (e.g., each neuron might only be allowed to form a new synapse if it has free “slots” or if an old weak synapse can be replaced), to avoid a combinatorial explosion of connections. But evidence suggests brains do form new synapses during learning, especially early in development or when learning new modalities, supporting our inclusion of this mechanism .

Pruning and Refinement: On the flip side, structural plasticity entails pruning of neurons or connections that are not useful. If a neuron has not fired for a very long time (perhaps the pattern it learned has ceased to appear in the environment), the network might remove it or repurpose it. Likewise, synapses that carry no significant weight (virtually zero) can be pruned to simplify the network. Pruning not only prevents waste of resources, it can also improve generalization by removing overfit micro-connections. In our multimodal association example, after the association phase, each neuron was said to prune 90% of possible connections, keeping only the strongest that matter . This drastically improves efficiency and enforces that each neuron is strongly connected only to the features that define its learned pattern. We would apply similar criteria continually or in phases – e.g., after an initial training period, go through a pruning phase to cut weak links, then continue learning.

Neurogenesis and Apoptosis Analogy: One can view these additions and deletions like neurogenesis (birth of new neurons) and apoptosis (programmed cell death) in the brain. Early in learning, there may be a rapid growth of synapses and neurons to map out the sensory environment. Once the network “knows” the basics, it might enter a consolidation phase where excess connections are removed and only stable circuits remain. Then, encountering a new environment or new stimuli might spark targeted growth again. Throughout, a small amount of background plasticity keeps refining connections (akin to adult plasticity).

Structural Plasticity in Implementation: To implement growing new neurons in a simulation, we can dynamically allocate new units in the model data structures when needed. One challenge is initializing their incoming and outgoing synapses; a reasonable approach is random small weights (so they start neutral) or copy an existing neuron and add some noise (so that it’s similar to something but can diverge). We prefer random to avoid bias. For synapse sprouting, implementation can be event-driven: whenever two neurons have a high correlation above a threshold and they lack a connection, create one with an initial weight. This could be done by monitoring a correlation matrix with a decay (so it tracks recent co-activations). Pruning can be done periodically by checking all weights and removing those below a threshold, and possibly removing neurons that have all weights pruned. Data structures should allow neurons to be indexed and iterated efficiently even as the size changes (e.g., using lists of neuron objects rather than fixed-size matrices, or allocating a maximal size and marking unused slots).

Maintaining Stability During Structural Changes: We must ensure that adding or removing elements doesn’t destabilize what’s already learned. When a new neuron is added, we might freeze its neighbors’ learning for a short while or introduce it slowly (low learning rate initially) so it doesn’t steal input from others abruptly. When pruning, ensure that the network’s performance (e.g., prediction accuracy) isn’t dramatically impacted; maybe prune only those that clearly have minimal effect. A conservative approach: mark synapses for removal, but actually remove them only if the network performs okay without them (trial removal). These details ensure structural plasticity improves the network progressively rather than causing oscillations.



In summary, structural plasticity gives the system a form of open-ended learning capability. It will not be limited by a fixed architecture if the data presents new challenges. Instead, it can self-organize not just in terms of weights but also in wiring and neuron count. This is vital for a lifelong learning AI that might start tabula rasa and continually grow in a rich environment.



Monitoring and Evaluating Representation Development


Since our system learns without explicit external feedback, it is important to have internal or diagnostic methods to monitor its progress and ensure that useful representations are forming (and to debug or adjust if not). We outline several approaches to evaluate the internal state and development of the network over time, all without requiring labeled data or human-defined benchmarks:

Prediction Error Trends: A primary self-evaluation metric is the prediction error for each modality. The system can measure how well its predicted next audio frame matches the actual next audio input (and similarly for video) by comparing the activation of expected neurons to those actually stimulated. Over time, as the network learns temporal patterns, these prediction errors (e.g., squared difference between predicted and actual sensory signal, or number of unexpected active units) should decrease. Monitoring the decrease in prediction error over training time gives a sense of how much structure the network has captured. If error plateaus at a high value, it may indicate the network capacity or architecture isn’t sufficient to model the remaining complexity, or that learning parameters need tuning. A well-functioning model should show a steep drop in error early on (learning basic patterns), then gradually lower errors as it picks up more subtle regularities.

Diversity of Learned Features: We can periodically inspect the receptive fields or weight patterns of neurons to see what they have learned. For the visual pathway, one can visualize each neuron’s incoming weights as an image (by mapping weights to pixel intensities or a patch). Often, unsupervised learning yields interpretable filters – e.g. one might see oriented edge-like patterns or color opponency in those weight visualizations. For the auditory pathway, one can interpret a neuron’s weights in the frequency domain: plotting the weight magnitude for each frequency channel to see if it’s tuned to low vs high frequencies, or a combination (it might show a band-pass characteristic or multiple peaks indicating sensitivity to specific tone pairs). If the network is developing well, we expect to see a variety of receptive fields: some edge detectors at various angles, some motion-sensitive units, some frequency-tuned auditory units, some onset detectors, etc. If instead we find many neurons have very similar weight patterns, that’s a red flag that representational collapse or insufficient competition occurred. Tools from neuroscience, like independent component analysis of weights or clustering analysis, can quantify how diverse the set of learned filters is. Additionally, we might track the effective dimensionality of the representations: e.g., perform PCA on the activation patterns of all neurons over a long sequence and see how many principal components dominate. A high number indicates a rich representation; a low number (much lower than number of neurons) might indicate redundancy.

Activation Sparsity and Utilization: Monitoring the sparsity of neural activations is useful. We log what fraction of neurons are active at once per layer (ideally a small fraction, e.g. 5-10%) and also track the distribution of firing rates across neurons. We want each neuron to have some moderate level of activity over time (no neuron should be completely inactive forever, and none should be saturating always on). A histogram of activation frequencies can show if the network has a healthy spread (most neurons firing occasionally, with perhaps a heavy-tail distribution). If we see neurons that never fire, we might consider lowering their thresholds or, if structural plasticity is enabled, eventually removing or resetting them. If we see the average fraction active creeping up (network getting too “busy”), we might strengthen inhibition or thresholds to push it back down. Homeostatic target rates can be monitored per neuron – each neuron could maintain an estimate of its recent average activity and we ensure it stays around a target (e.g., 0.1 firing probability). Observing these over time confirms whether homeostasis is working (they should converge to target). Any neuron far off target indicates an adaptation issue.

Emergence of Multimodal Associations: To evaluate if audio-visual alignment is happening, we can analyze the correlation between modality representations. For instance, compute the mutual information or correlation between the firing of each visual feature neuron and each auditory feature neuron. Initially, these should be near zero (randomly initialized, no connections). As training proceeds, we expect some pairs (or some multimodal neurons) to show significantly high correlation – those likely correspond to discovered audio-visual links. We could track the number of cross-modal synapses that have been created and their strengths as a function of time. An increase indicates the system is indeed finding cross-modal regularities. We might also perform a simple ablation test: feed the system one modality alone (say, show the video without sound) and see if the auditory neurons for the associated sound still activate via learned associations. If yes, that’s a clear sign of multimodal memory. Similarly, play the sound alone and see if it triggers the visual neurons. These internal “cued recall” tests can be done by clamping one modality’s input and observing the other modality’s activity in the absence of external input. Such tests need to be done carefully to not interfere with learning (or done in a testing phase with learning off).

Visualization of Representational Space: We can take snapshots of the internal state vectors over time and use dimensionality reduction (like t-SNE or PCA) to project them into a 2D plot. Each point could represent the state of the high-level layer for one time step. Coloring these points by some ground truth event label (if we have it for analysis, though not for learning) can show if the network unwittingly clustered similar events together. Even without ground truth, we might see clustering of states, which indicates the network has formed distinct representations for different kinds of input scenarios. We can also examine trajectories in this state space for recurring sequences.

Assessment with Downstream Probes (optional): Although the system is unsupervised, we can periodically freeze its learned parameters and train a simple readout (like a linear classifier or KNN) on an independent set of labeled data to see how well the unsupervised features classify or cluster those labels. For example, after training on raw webcam and mic data, we might present it with a series of known events (like a person performing certain actions with sounds, labeled for evaluation) and see if the internal representation separates these events distinctly. If the features are good, even a simple linear probe should achieve decent classification accuracy. This kind of evaluation, often done in self-supervised learning research, tells us if the model has captured semantically useful structure. However, we emphasize that this is just for analysis – the system itself never uses these labels.

Monitoring Structural Changes: Keep track of how many neurons and synapses exist over time. A well-tuned structural plasticity mechanism might show a rapid growth at the beginning (many new synapses forming as the network discovers structure), followed by a pruning phase (useless connections removed), and then a slower, occasional growth as new patterns are encountered. If neuron count keeps growing without bound, it might mean the novelty threshold is too low or the network isn’t consolidating knowledge (possibly overfitting every minor variation). If neuron count stays too low and prediction error remains high, maybe the network is under-capacity and should grow more. This dynamic can be monitored and parameters adjusted accordingly (like thresholds for adding neurons or the learning rate for new neurons).

Receptive Field Evolution: By saving the model’s state at different time points, we can literally observe how a given neuron’s receptive field (its weight vector) changes over time. Early on, it might be random; then it starts aligning to some structure as a rough draft; eventually it fine-tunes to a stable pattern. Plotting a neuron’s weight vector at intervals can reveal if it converged. Ideally, most neurons should converge to stable receptive fields that don’t change much anymore once the network has seen enough data (except if the environment statistics change). If some neuron’s weights keep oscillating or never settle, it may indicate instability or an ambiguous feature (perhaps trying to represent two patterns alternately). We might then increase competition or split that neuron into two to resolve the conflict.

Energy or Cost Monitoring: Although we don’t have a single scalar loss (no backprop loss), we can define an energy function like the sum of squared prediction errors or the negative of total Hebbian co-activation strength (for instance, Hopfield networks have an energy that decreases as they learn associations). Monitoring such a global measure (if definable) could be insightful. Typically, unsupervised learning is seen as maximizing explained variance or minimizing surprise. We might measure the surprise at each time (inverse of prediction confidence) and see it decrease overall.



By using these monitoring methods, developers of this AI system can verify that the self-organizing process is working as intended – i.e., the system is learning progressively more structured and stable representations, without diverging into instability. Furthermore, these methods help in guiding hyperparameter choices (for example, if we see too many dead neurons, we adjust the homeostatic target or learning rate).



Finally, it’s worth noting that while the system has no external evaluation during learning, in a developmental context one could imagine evaluating its behavior by how well it can reconstruct or predict the sensory input. If needed, one could allow the system to “close the loop” and generate its predicted next frame as an output to compare with the actual next frame, essentially training a generative model. But since we avoid any global loss, we rely on the described local signals and monitoring to shape learning.



Conclusion


In this extended architecture, we have designed a completely self-driven audio-visual learning system that takes inspiration from biological brains. It starts as a blank slate with no knowledge of vision or hearing, and through local plasticity rules it bootstraps itself into a sophisticated state: neurons in early layers respond to simple visual and auditory patterns, higher-layer neurons capture temporal sequences and multimodal associations, and the entire network dynamically grows and adapts to the environment. The system learns joint audio-visual representations purely from the raw stream, aligning the senses where appropriate while keeping them separate when not. It also inherently learns to predict future sensory events, which serves as a built-in self-supervised objective to refine its internal model of the world.



All of this is achieved without any backpropagation or manual labels, relying instead on principles like Hebbian learning (“cells that fire together wire together”), normalization to maintain balance, and structural plasticity for open-ended growth. These principles have strong grounding in neuroscience – they reflect how real neural circuits self-organize to form maps and associative memories . For example, our use of STDP to link modalities is supported by experiments showing that neurons can learn cross-modal mappings via timing-based plasticity . The emphasis on stability through homeostasis is echoed by the brain’s regulation of synaptic strength to avoid runaway excitation . And the overall idea of a convergence-divergence zone for multimodal integration comes from cognitive neuroscience theories of memory and perceptual binding .



In implementation terms, this architecture can be realized with spiking neural network models (leveraging STDP and lateral inhibition circuits) or with rate-based neural models using equivalent local rules (Hebbian updates with normalization, etc.). Modern research is increasingly exploring such brain-like learning algorithms. For instance, Ferré et al. (2018) demonstrated multilayer unsupervised feature learning with STDP and showed the necessity of competition and normalization for convergence . Khacef et al. (2021) developed the Reentrant SOM model which uses local Hebbian links to perform multimodal association, achieving near-supervised accuracy with only a handful of labels by capitalizing on self-organized representations . Very recently, Ravichandran et al. (2025) incorporated both Hebbian synaptic plasticity and structural plasticity in a deep network and reported competitive results without backprop , underscoring that such approaches are viable for real-world tasks. These works provide encouragement that our proposed design, while ambitious, is grounded in feasible techniques.



In conclusion, by combining unimodal self-organization, cross-modal Hebbian association, predictive sequence learning, and rigorous stability controls, the system should autonomously develop a rich internal model of the audio-visual world. It will transform raw pixels and waveforms into a web of interconnected features, akin to how a young brain makes sense of sights and sounds. This design opens the path toward truly self-organizing AI that learns from life experience, not from human labeling, aligning with the long-term vision of autonomous cognitive development in machines.



Sources:

Hebbian and STDP learning rules enabling unsupervised receptive field formation .

Winner-take-all and weight normalization strategies for stable unsupervised feature learning .

Independent component analysis as a unifying principle for early sensory coding in both vision and audition .

Reentrant multimodal association via Hebbian links and structural plasticity (sprouting/pruning) .

STDP-driven mapping between sensory maps for multimodal integration and sequence learning in recurrent networks .

Predictive coding concept applied to video frames (each layer forwards only prediction errors) .

Homeostatic plasticity mechanisms to prevent runaway excitation in Hebbian learning .

Convergence-divergence zones theory of multimodal memory (binding sight and sound) .
